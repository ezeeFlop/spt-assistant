version: '3.8'

services:
  redis:
    image: redis:7-alpine
    hostname: redis
    volumes:
      - redis_data:/data
    networks:
      - voice_assistant_net
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager] # Or specific worker nodes

  api:
    image: registry.sponge-theory.dev/spt-assistant-api:latest 

    hostname: api
    ports:
      - "8301:8301"
    networks:
      - voice_assistant_net
      - webfacing
    depends_on:
      - redis
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - VITE_API_BASE_URL=${VITE_API_BASE_URL}
      # - LOG_LEVEL=INFO # Example, if your app uses this
    deploy:
      replicas: 1 # Or more for scalability
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.labels.GPU == false

  vad_stt_worker:
    image: registry.sponge-theory.dev/spt-assistant-vad-stt:latest 

    hostname: vad_stt_worker
    networks:
      - voice_assistant_net
    depends_on:
      - redis
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - STT_MODEL_NAME=${STT_MODEL_NAME}
      # - STT_DEVICE=cpu # Ensure this aligns with your Dockerfile (cpu or cuda)
      # - LOG_LEVEL=INFO
    # volumes: # Example if you need to mount .env or models not in image
      # - ./vad_stt_worker/.env:/app/vad_stt_worker/.env
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.labels.GPU == true

  llm_orchestrator_worker:
    image: registry.sponge-theory.dev/spt-assistant-llm-orchestrator:latest 

    hostname: llm_orchestrator_worker
    networks:
      - voice_assistant_net
      - ollama
    depends_on:
      - redis
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LLM_BASE_URL=http://spt-smi_ollama:11434
      - NLTK_DATA=/usr/share/nltk_data
      # - LLM_API_KEY=your_llm_api_key_here # Use Docker secrets for production
      # - LOG_LEVEL=INFO
    # volumes:
      # - ./llm_orchestrator_worker/.env:/app/llm_orchestrator_worker/.env
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.labels.GPU == false
          
  tts_worker:
    image: registry.sponge-theory.dev/spt-assistant-tts:latest

    hostname: tts_worker
    networks:
      - voice_assistant_net
    depends_on:
      - redis
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      # - LOG_LEVEL=INFO
      # - TTS_PROVIDER=piper # if configurable
      # - PIPER_EXECUTABLE_PATH=/app/tts_worker/piper_tts/piper # if bundled and path needs to be set
      # - PIPER_VOICES_DIR=/app/tts_worker/piper_tts/voices # if bundled
    # volumes: # Example for Piper models/executable if not in the image, or for .env
      # - ./tts_worker/.env:/app/tts_worker/.env
      # - /path/to/local/piper_executable:/app/tts_worker/piper_tts/piper 
      # - /path/to/local/piper_voices:/app/tts_worker/piper_tts/voices
    volumes:
        - /data/smi/prod/data:/data
        - /data/smi/prod/cache:/app/.cache
        - /data/smi/prod/cache:/root/.cache
        - /data/smi/prod/cache:/root/.local/share
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.labels.GPU == true

volumes:
  redis_data:

networks:
  voice_assistant_net:
    driver: overlay
    attachable: true
  webfacing:
    driver: overlay
    external: true
  ollama:
    driver: overlay
    external: true