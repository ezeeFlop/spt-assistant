# llm_orchestrator_worker/.env.example

# Redis Settings
REDIS_HOST=localhost
REDIS_PORT=6379
# REDIS_DB=0
# REDIS_PASSWORD=your_redis_password

# Logging Level
LOG_LEVEL=INFO # DEBUG, INFO, WARNING, ERROR, CRITICAL

# --- LLM Configuration ---

# Option 1: Using a remote provider like OpenAI
# LLM_PROVIDER="openai"
# LLM_MODEL_NAME="gpt-4o" # Or "gpt-3.5-turbo", etc.
# LLM_API_KEY="sk-YOUR_OPENAI_API_KEY" # Replace with your actual OpenAI API key
# LLM_BASE_URL= # Typically not needed for OpenAI unless using a proxy

# Option 2: Using a local Ollama setup (via LiteLLM)
LLM_PROVIDER="ollama"
LLM_MODEL_NAME="llama3.1" # Replace with your pulled Ollama model (e.g., mistral, llama3.1:8b)
LLM_API_KEY="ollama" # Placeholder, as Ollama doesn't typically require a key via LiteLLM if LLM_BASE_URL is set
LLM_BASE_URL="http://localhost:11434" # Default Ollama API base. LiteLLM will append /v1.

# Option 3: Other LiteLLM supported provider (e.g., Anthropic)
# LLM_PROVIDER="anthropic"
# LLM_MODEL_NAME="claude-3-opus-20240229"
# ANTHROPIC_API_KEY="sk-YOUR_ANTHROPIC_API_KEY" # LiteLLM picks this up from environment if prefixed correctly
# LLM_API_KEY="sk-YOUR_ANTHROPIC_API_KEY" # Can also be set via LLM_API_KEY directly
# LLM_BASE_URL= # Usually not needed for major cloud providers

# General LLM Parameters
LLM_MAX_TOKENS=1500
LLM_TEMPERATURE=0.7
MAX_CONVERSATION_HISTORY=10 # Number of turns (user + assistant) to keep
SYSTEM_PROMPT="You are Tara, a helpful and concise voice assistant. Respond in French unless the user speaks in another language."

# Default TTS voice if not specified in conversation config
# This should match a voice ID available in your configured TTS service (Piper or Coqui)
DEFAULT_TTS_VOICE_ID="fr_FR-siwis-medium.onnx" # Example for Piper
# DEFAULT_TTS_VOICE_ID="tts_models/fr/fairseq/vits--unique_speaker" # Example for Coqui (model_name--speaker_id)

# Path to MCP client config (if used for tool definitions)
# MCP_CLIENT_CONFIG_PATH=